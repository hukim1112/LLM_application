{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/LLM_application/blob/main/3_Langchain_opensourceLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install langchain-openai\n",
        "!pip install langchain_ollama\n",
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "2fg8PYQil-Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 다운로드합니다."
      ],
      "metadata": {
        "id": "EBtIH7ZOl69z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download \\\n",
        "  heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF \\\n",
        "  ggml-model-Q5_K_M.gguf \\\n",
        "  --local-dir ./ \\\n",
        "  --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "3T-T3lWtfxLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "스크립트로 ollama를 설치합니다."
      ],
      "metadata": {
        "id": "LEy0fibvmjwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "710lfsjzfxQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelfile을 구글 드라이브에서 다운 받습니다. EEVE 모델의 프롬프트 템플릿 정보를 갖고 있습니다."
      ],
      "metadata": {
        "id": "axdrGKcAmnY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://drive.google.com/uc?export=download&id=14uDtG4Y5l6o1sTH3MVMWkyQSuO8RiRA8' -O /content/Modelfile"
      ],
      "metadata": {
        "id": "HVpkbtAKmSJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ollama에서 EEVE 모델을 서빙합니다.  nohup 명령어는 프로세스를 백그라운드에서 실행합니다."
      ],
      "metadata": {
        "id": "skr4efymmvZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve & #ollama server 실행"
      ],
      "metadata": {
        "id": "YyGvdr2Sm14C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama create EEVE-Korean-10.8B -f Modelfile #gguf => 모델"
      ],
      "metadata": {
        "id": "QXI5pUMxfxwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list #현재 모델 리스트"
      ],
      "metadata": {
        "id": "H8ptGNm4lEX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgCTIjkMobXk"
      },
      "source": [
        "# Ollama 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjDsLYgcobXl"
      },
      "source": [
        "ollama로 모델을 실행시킨 뒤 연결해야 함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f7Nr1MWobXl"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# 올바른 URL과 인증 정보 설정\n",
        "llm = ChatOllama(\n",
        "    model=\"EEVE-Korean-10.8B:latest\",\n",
        "    api_base_url=\"http://localhost:8000\",  # 서버 URL\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXljdycrobXm"
      },
      "outputs": [],
      "source": [
        "llm.invoke(\"안녕\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HWx02DXobXm"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
        "\n",
        "# LangChain 표현식 언어 체인 구문을 사용합니다.\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# 간결성을 위해 응답은 터미널에 출력됩니다.\n",
        "# 프로덕션 환경에서 애플리케이션을 배포하기 위해 LangServe를 사용할 수 있습니다.\n",
        "response = chain.invoke({\"topic\": \"deep learning\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMx_nd5uobXn"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FTTBvdIobXn"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "template = \"\"\"Answer the following question in Korean.\n",
        "#Question:\n",
        "{question}\n",
        "\n",
        "#Answer: \"\"\"  # 질문과 답변 형식을 정의하는 템플릿\n",
        "prompt = PromptTemplate.from_template(template)  # 템플릿을 사용하여 프롬프트 객체 생성\n",
        "\n",
        "# 프롬프트와 언어 모델을 연결하여 체인 생성\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "question = \"대한민국의 수도는 어디야?\"  # 질문 정의\n",
        "\n",
        "print(\n",
        "    chain.invoke({\"question\": question})\n",
        ")  # 체인을 호출하여 질문에 대한 답변 생성 및 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xap-hh2-obXn"
      },
      "outputs": [],
      "source": [
        "email_conversation = \"\"\"안녕하세요, 저는 김민수라고 합니다. 나이는 35세이고 현재 서울에 거주하고 있습니다.\n",
        "현재 APEX Solutions라는 IT 회사에서 근무하고 있으며, 연봉은 세전 기준으로 약 6,500만 원입니다.\n",
        "대출을 받고자 하는 이유는 신혼집 마련을 위해 아파트를 구매하려는 목적입니다.\n",
        "현재 매매하려는 아파트의 가격은 5억 원 정도이고, 제 자산 중 약 1억 5천만 원을 계약금으로 준비할 수 있습니다.\n",
        "따라서 대출로 약 3억 5천만 원을 실행하려고 합니다.\n",
        "\n",
        "저는 신용등급이 2등급으로 안정적이고, 지난 10년 동안 큰 금융 거래 문제 없이 생활해왔습니다.\n",
        "현재 대출 상담을 받으려는 이유는 금리와 상환 조건을 미리 확인하고, 저에게 적합한 대출 상품을 찾고 싶어서입니다.\n",
        "가능한 한 상환 기간을 20년으로 설정하고, 중간에 일부 상환이 가능하도록 설계된 상품을 찾고 싶습니다.\n",
        "또, 대출 실행이 가능한 시점과 관련해 필요한 서류와 절차도 알고 싶습니다.\n",
        "\n",
        "추가로, 제가 다니고 있는 회사에서 곧 승진 기회가 있어 연봉이 조금 더 오를 가능성도 있습니다.\n",
        "이 점이 대출 한도나 조건에 긍정적인 영향을 줄 수 있을지 궁금합니다. 감사합니다.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm1zjRcEobXo"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\n",
        "    \"다음의 이메일 내용중 중요한 내용을 추출해 주세요.\\n\\n{email_conversation}\"\n",
        ")\n",
        "\n",
        "\n",
        "response = llm.invoke(template.invoke({\"email_conversation\" : email_conversation}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSshwYZEobXo"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoKsb40ZobXo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "# JSON Output Parser 정의\n",
        "class JsonOutputParser(BaseOutputParser):\n",
        "    def get_format_instructions(self) -> str:\n",
        "        \"\"\"JSON 형식 출력에 대한 설명을 반환\"\"\"\n",
        "        return \"\"\"\n",
        "Please provide only the output in the following JSON format, with no additional text, comments, or extra characters:\n",
        "\n",
        "{\n",
        "  \"name\": \"string\",\n",
        "  \"age\": \"integer\",\n",
        "  \"company\": \"string\",\n",
        "  \"salary\": \"integer\",\n",
        "  \"loan_amount\": \"integer\",\n",
        "  \"purpose\": \"string\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "    def parse(self, output: str) -> dict:\n",
        "        \"\"\"JSON 형식의 문자열을 파싱\"\"\"\n",
        "        try:\n",
        "            return json.loads(output)\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Invalid JSON format: {e}\")\n",
        "\n",
        "# Parser 생성\n",
        "parser = JsonOutputParser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9G4Txq5obXo"
      },
      "outputs": [],
      "source": [
        "structured_output = parser.parse(\"\"\"{\n",
        "  \"name\": \"김민수\",\n",
        "  \"age\": 35,\n",
        "  \"company\": \"APEX Solutions\",\n",
        "  \"salary\": 65000000,\n",
        "  \"loan_amount\": 35000000,\n",
        "  \"purpose\": \"신혼집 마련을 위한 아파트 구매(5억원)\"\n",
        "}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvodXfU5obXp"
      },
      "outputs": [],
      "source": [
        "structured_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vauAufBobXp"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "# 템플릿 생성\n",
        "template = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "You are a helpful assistant. Please answer the following questions in KOREAN.\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "EMAIL CONVERSATION:\n",
        "{email_conversation}\n",
        "\n",
        "FORMAT:\n",
        "{format}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# JSON 형식의 출력 포맷 설정\n",
        "template = template.partial(format=parser.get_format_instructions())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO7X6xvnobXp"
      },
      "outputs": [],
      "source": [
        "prompt = template.invoke({\"question\" : \"이메일 내용을 FORMAT 형식으로 생성해주세요.\", \"email_conversation\" : email_conversation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysw3tAj6obXp"
      },
      "outputs": [],
      "source": [
        "print(prompt.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMZTRCZPobXp"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc546X5WobXp"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTIsh-BPobXp"
      },
      "outputs": [],
      "source": [
        "structured_output = parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cWM8qPhobXp"
      },
      "outputs": [],
      "source": [
        "print(structured_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It9U4IQbobXq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_miVoOBobXq"
      },
      "source": [
        "# vLLM 활용(A100 이상 필요)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14tVthXobXq"
      },
      "source": [
        "미니프로젝트 주제 : vLLM으로 호스팅된 Huggingface 기반 한국어 모델(Llama 3)을 활용하여 LangChain으로 Chain을 설계하고, 이를 기반으로 실제 사용할 수 있는 LLM 앱을 개발합니다.\n",
        "\n",
        "1. vLLM으로 모델을 호스팅 (필요 시 ngrok을 사용해 특정 도메인으로 api 호출이 가능하도록 할 것)\n",
        "\n",
        "2. PromptTemplates를 디자인하고, Chain을 구성\n",
        "\n",
        "3. LangChain으로 response를 잘 생성하는 지 확인하기\n",
        "\n",
        "4. Gradio Chat UI에 연결하여 대화해보기."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM으로 모델 호스팅 (LangChain과 연결)"
      ],
      "metadata": {
        "id": "nM--boxsDcuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  vllm -q"
      ],
      "metadata": {
        "id": "blmhmc9sDSuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 다운로드(수 분 이상 걸릴 수 있음) 및 vLLM 서버 구동"
      ],
      "metadata": {
        "id": "LipXvTcWiLuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup vllm serve Qwen/Qwen2.5-7B-Instruct > nohup_script.out &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjJ2SA2VIuuV",
        "outputId": "bc81e4aa-2360-4179-f842-93ba22c36914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Curl 명령어 테스트 (vLLM의 api 서버 사용)"
      ],
      "metadata": {
        "id": "5NOyxUgaQ1st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "# curl 명령어를 Python에서 실행\n",
        "command = '''curl http://localhost:8000/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. you will only respond in Korean.\"},\n",
        "        {\"role\": \"user\", \"content\": \"내 이름은 김형욱입니다.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"안녕하세요, 김형욱님! 만나서 반갑습니다. 궁금한 사항이나 도와드릴 일이 있으신가요?\"},\n",
        "        {\"role\": \"user\", \"content\" : \"내 이름 기억해?\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_p\": 0.8,\n",
        "    \"repetition_penalty\": 1.05,\n",
        "    \"max_tokens\": 512\n",
        "  }' '''  # 마지막 작은따옴표 수정\n",
        "\n",
        "# subprocess를 사용하여 curl 명령어 실행\n",
        "result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "# 결과 출력\n",
        "print(result.stdout)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqYxj0R5PJVJ",
        "outputId": "939cbf40-3b0a-4a5d-f26b-4052b1dc1ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"id\":\"chatcmpl-050715bf2e5e4c3fab4b26fceedd3146\",\"object\":\"chat.completion\",\"created\":1734483160,\"model\":\"Qwen/Qwen2.5-7B-Instruct\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"네, 김형욱님이라는 이름을 기억하고 있습니다. 언제든지 도움이 필요하시면 말씀해주세요.\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":91,\"total_tokens\":117,\"completion_tokens\":26,\"prompt_tokens_details\":null},\"prompt_logprobs\":null}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI Client를 사용한 질의요청\n",
        "\n",
        "vLLM은 LLM과 상호작용할 때, OpenAI의 api 포맷을 그대로 지원함."
      ],
      "metadata": {
        "id": "Fvlzb9roQ3v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "# Set OpenAI's API key and API base to use vLLM's API server.\n",
        "openai_api_key = \"EMPTY\"\n",
        "openai_api_base = \"http://localhost:8000/v1\"\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    base_url=openai_api_base,\n",
        ")\n",
        "\n",
        "chat_response = client.chat.completions.create(\n",
        "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. you will only respond in Korean.\"},\n",
        "        {\"role\": \"user\", \"content\": \"내 이름은 김형욱입니다.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"안녕하세요, 김형욱님! 만나서 반갑습니다. 궁금한 사항이나 도와드릴 일이 있으신가요?\"},\n",
        "        {\"role\": \"user\", \"content\" : \"내 이름 기억해?\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    top_p=0.8,\n",
        "    max_tokens=512,\n",
        "    extra_body={\n",
        "        \"repetition_penalty\": 1.05,\n",
        "    },\n",
        ")\n",
        "print(\"Chat response:\", chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z0OZQRpQyAm",
        "outputId": "e4c83280-f2eb-4de4-9aeb-a824a7d33806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat response: 네, 김형욱님이라는 이름을 기억하고 있습니다. 언제든지 도움이 필요하시면 말씀해 주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain와 vLLM 연결\n",
        "\n",
        "- openai_api_base에 api 서버의 주소를 입력. 현재 localhost의 8000번 포트를 사용 중."
      ],
      "metadata": {
        "id": "tk2pGB9MEKqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=\"EMPTY\",\n",
        "    openai_api_base=\"http://localhost:8000/v1\",\n",
        "    model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    temperature=0.7,\n",
        "    top_p=0.8,\n",
        "    frequency_penalty=1.05,\n",
        "    max_tokens=512\n",
        ")\n",
        "print(llm.invoke(\"내 이름은 김형욱이라고 합니다.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whhXDIEWQyC3",
        "outputId": "379a0684-a76a-4d85-da49-5993cc1243ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='안녕하세요, 김형욱님! 만나서 반갑습니다. 저는 Qwen이라고 합니다. 알리바바 클라우드에서 만든 언어 모델입니다. 오늘 어떤 도움이 필요하신가요? 궁금한 점이나 이야기 나누고 싶은 주제가 있으시다면 말씀해 주세요.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 38, 'total_tokens': 112, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen2.5-7B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-0fc9834c-08fb-4fa3-87fe-e9b9a808ece9-0' usage_metadata={'input_tokens': 38, 'output_tokens': 74, 'total_tokens': 112, 'input_token_details': {}, 'output_token_details': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain의 ChatOpenAI 클래스로 연결"
      ],
      "metadata": {
        "id": "9VT-AG8pRKfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
        "\n",
        "llm.invoke(\n",
        "    [\n",
        "        SystemMessage(content=\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. you will only respond in Korean.\"),\n",
        "        HumanMessage(content=\"내 이름은 김형욱입니다.\"),\n",
        "        AIMessage(content=\"반가워요, 오늘 어떻게 도와드릴까요?\"),\n",
        "        HumanMessage(content=\"한국의 버팀목 전세대출에 대해 설명해줘.\")\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNJwZ9J3QyFg",
        "outputId": "308f0287-2db0-4257-ea5b-379b1543742c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='버팀목 전세대출은 주택 구매를 돕기 위한 방법 중 하나로, 주택 구매자의 성립되지 않은 신용이나 낮은 소득으로 대출을 받기 어렵다고 판단되는 경우에 제공되는 대출입니다. 이 프로그램은 주로 저소득 가구나 신용 기록이 부족한 이들에게 전세 입주를 돕는 데 사용됩니다.\\n\\n버팀목 전세대출의 주요 특징은 다음과 같습니다:\\n\\n1. 보증금과 월세 보조: 대출은 보증금과 월세를 지원합니다. 이는 주택을 구매하는 데 필요한 초기 비용을 지원합니다.\\n\\n2. 정부 지원: 이 대출은 주로 정부가 지원하는 프로그램으로, 대출금 상환을 보증하거나 향후 대출금을 주택 구매자에게 반환하도록 돕는 역할을 합니다.\\n\\n3. 대출 조건: 대출 조건은 일반적으로 신용 점수, 소득, 재무 상황 등을 기준으로 합니다. 저소득층이나 신용 기록이 부족한 이들을 대상으로 합니다.\\n\\n4. 대출 기간: 대출 기간은 일반적으로 3년에서 5년 사이이며, 이 기간 동안 대출은 보증금과 월세로만 사용될 수 있습니다.\\n\\n5. 대출 상환: 대출은 주택을 구매한 후 신청자가 주택을 판매하거나 대출을 직접 상환할 때까지 계속됩니다.\\n\\n이러한 버팀목 전세대출은 주택 구매를 희망하는 저소득층이나 신용 기록이 부족한 이들에게 큰 도움이 될 수 있습니다. 하지만, 구체적인 조건과 절차는 지역별로 다르므로, 지역별로 제공되는 정확한 정보를 확인하시기 바랍니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 420, 'prompt_tokens': 82, 'total_tokens': 502, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'Qwen/Qwen2.5-7B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2b31128f-2159-4ae2-9994-8e1de8b73a85-0', usage_metadata={'input_tokens': 82, 'output_tokens': 420, 'total_tokens': 502, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD_4jvjtobXq"
      },
      "source": [
        "### Reference\n",
        "\n",
        "LangChain으로 원격에 있는 vLLM 서버(openAI api 스타일)로 챗봇 연결 [link](https://python.langchain.com/docs/integrations/chat/vllm/)\n",
        "\n",
        "vLLM+LangChain [link](https://python.langchain.com/docs/integrations/llms/vllm/)\n",
        "\n",
        "vLLM complete guide [link](https://lunary.ai/blog/vllm-langchain-tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gt_wgduobXq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6fOUKjxobXq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKSi9kGvobXq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HtSWpRQobXq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python langchain",
      "language": "python",
      "name": "langchain"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}