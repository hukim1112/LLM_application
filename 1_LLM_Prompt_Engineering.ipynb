{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/LLM_application/blob/main/1_LLM_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-s6fELs-QoK"
      },
      "source": [
        "# Getting Started with Prompt Engineering\n",
        "\n",
        "\n",
        "# 프롬프트 엔지니어링(Prompt Engineering) 소개\n",
        "\n",
        "## 1. 프롬프트 엔지니어링이란?\n",
        "\n",
        "프롬프트 엔지니어링(Prompt Engineering)은 대규모 언어 모델(LLM, Large Language Model)의 성능을 최적화하기 위해 입력 문장을 전략적으로 설계하는 과정입니다. 이는 단순히 모델에게 질문을 던지는 것이 아니라, 원하는 결과를 얻기 위해 적절한 형식과 구조를 갖춘 입력을 구성하는 것을 의미합니다. LLM은 입력된 텍스트를 바탕으로 확률적으로 다음 단어를 예측하여 출력을 생성하는 방식으로 동작하므로, 입력 방식에 따라 출력 품질이 크게 달라질 수 있습니다.\n",
        "\n",
        "## 2. 프롬프트 엔지니어링이 중요한 이유\n",
        "\n",
        "1. **출력의 정확성과 일관성 향상**: 단순한 질문보다 구조화된 프롬프트를 사용하면 보다 일관되고 신뢰할 수 있는 결과를 얻을 수 있습니다.\n",
        "2. **모델 활용 최적화**: 특정 패턴을 가진 입력을 사용하면 원하는 유형의 응답을 더 쉽게 얻을 수 있습니다. 이는 모델을 별도로 Fine-tuning하지 않고도 성능을 개선할 수 있는 방법이기도 합니다.\n",
        "3. **작업 자동화와 응용 분야 확장**: 코드 생성, 데이터 요약, 질의응답 시스템 등 다양한 AI 응용 사례에서 효과적인 프롬프트 디자인이 핵심적인 역할을 합니다.\n",
        "4. **비용 절감**: 모델 호출 횟수를 줄이고 처리 속도를 높이며, 모델이 불필요한 출력을 하지 않도록 유도하여 컴퓨팅 자원을 절약할 수 있습니다.\n",
        "\n",
        "## 3. 프롬프트 엔지니어링의 핵심 기법\n",
        "\n",
        "1. **명확한 지시 제공(Instruction Clarity)**: 모델이 명확히 이해할 수 있도록 구체적인 지시를 포함합니다.\n",
        "   - ❌ \"요약해줘.\"\n",
        "   - ✅ \"다음 텍스트를 한 문장으로 요약해줘.\"\n",
        "\n",
        "2. **컨텍스트 제공(Contextualization)**: 필요한 정보를 포함하여 모델이 더 정확한 출력을 하도록 유도합니다.\n",
        "   - ❌ \"이 제품에 대한 리뷰를 작성해줘.\"\n",
        "   - ✅ \"다음의 제품 설명을 참고하여 긍정적인 리뷰를 작성해줘.\"\n",
        "\n",
        "3. **예제 포함(Examples in Prompting)**: 모델이 참고할 수 있도록 예제를 제시합니다.\n",
        "   - ❌ \"문장을 바꿔 써줘.\"\n",
        "   - ✅ \"'나는 학교에 간다.'를 '나는 학업을 위해 학교를 방문했다.'처럼 바꿔줘.\"\n",
        "\n",
        "4. **제한과 형식 지정(Constraints & Formatting)**: 출력 형식을 명확히 정의하여 원하는 답변을 유도합니다.\n",
        "   - ❌ \"제품 비교를 해줘.\"\n",
        "   - ✅ \"A 제품과 B 제품을 기능, 가격, 사용자 리뷰의 세 가지 항목으로 비교하여 표 형식으로 정리해줘.\"\n",
        "\n",
        "5. **단계별 지시(Chain of Thought Prompting)**: 복잡한 문제를 해결할 때는 단계적으로 사고하도록 유도합니다.\n",
        "   - ❌ \"다음 수학 문제를 풀어줘: 23 x 17\"\n",
        "   - ✅ \"23과 17을 곱하는 과정을 단계별로 설명하면서 답을 구해줘.\"\n",
        "\n",
        "## 4. 결론\n",
        "\n",
        "프롬프트 엔지니어링은 단순한 입력 조작이 아니라, LLM을 효과적으로 활용하기 위한 필수적인 기술입니다. 적절한 프롬프트를 설계함으로써 모델의 성능을 극대화할 수 있으며, 이를 통해 AI 시스템을 보다 효율적이고 실용적으로 활용할 수 있습니다. 앞으로의 실습에서는 다양한 프롬프트 기법을 테스트하고, 최적의 방식을 탐색하는 과정을 다루겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSoHV3rW-QoN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IaogrYy-QoN"
      },
      "source": [
        "## 1. 기초 프롬프트 엔지니어링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wXqRsvA-QoN"
      },
      "source": [
        "필요한 라이브러리를 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lP-I8EO-QoO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# update or install the necessary libraries\n",
        "!pip install --upgrade openai\n",
        "!pip install --upgrade langchain\n",
        "!pip install langchain-community langchain-core\n",
        "!pip install --upgrade python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGujNVJr-QoP"
      },
      "source": [
        "API KEY는 환경 변수로 로드해야 합니다. python-dotenv로 .env 파일을 생성해서 일괄적으로 관리할 수도 있습니다. 또는 수동으로 API KEY를 환경 변수로 등록할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "d_kFgFsqmlTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU4lgawP-QoP"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 로드\n",
        "import openai\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 환경 변수 로드\n",
        "load_dotenv()\n",
        "\n",
        "# OpenAI API 설정\n",
        "openai.api_key = input(\"OpenAI API 키를 입력하세요: \")\n",
        "\n",
        "# LangChain 설정\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# OpenAI 클라이언트 초기화\n",
        "client = OpenAI(\n",
        "    api_key=openai.api_key,  # 여기에 OpenAI API 키를 입력하세요\n",
        ")"
      ],
      "metadata": {
        "id": "p_llBvUve_dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_openai_params(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\"\n",
        "    OpenAI 파라미터 설정\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"temperature\": temperature,\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"top_p\": top_p,\n",
        "        \"frequency_penalty\": frequency_penalty,\n",
        "        \"presence_penalty\": presence_penalty,\n",
        "    }\n",
        "    return params\n",
        "\n",
        "def get_completion(params, prompt):\n",
        "    \"\"\"\n",
        "    OpenAI ChatCompletion을 사용하여 응답 생성\n",
        "    \"\"\"\n",
        "    # 메시지 형식 수정\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # OpenAI API 호출\n",
        "    response = client.chat.completions.create(\n",
        "        model=params[\"model\"],\n",
        "        messages=messages,\n",
        "        temperature=params[\"temperature\"],\n",
        "        max_tokens=params[\"max_tokens\"],\n",
        "        top_p=params[\"top_p\"],\n",
        "        frequency_penalty=params[\"frequency_penalty\"],\n",
        "        presence_penalty=params[\"presence_penalty\"],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "M-lphPyYejpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "기본 프롬프트 예시:"
      ],
      "metadata": {
        "id": "qbq7vzE7kHh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트\n",
        "params = set_openai_params(temperature=0.7)\n",
        "prompt = \"하늘은\"\n",
        "\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"AI 응답:\", response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ],
      "metadata": {
        "id": "h29cEvVnim5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDkPQWD-QoQ"
      },
      "source": [
        "다른 temperature 세팅으로 비교"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_openai_params(temperature=0)"
      ],
      "metadata": {
        "id": "OlGQE974kZOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSgXz8B--QoQ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"AI 응답:\", response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ4JlwkI-QoR"
      },
      "source": [
        "### 1.1 Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNj6y5S5-QoR"
      },
      "outputs": [],
      "source": [
        "params = set_openai_params(temperature=0.7)\n",
        "prompt = \"\"\"항생제는 세균 감염을 치료하는 데 사용되는 약물입니다.\n",
        "세균을 죽이거나 증식을 막아 신체의 면역 체계가 감염을 퇴치할 수 있도록 돕습니다.\n",
        "항생제는 일반적으로 알약, 캡슐, 액체 용액 형태로 경구 복용하거나, 경우에 따라 정맥 주사로 투여됩니다.\n",
        "항생제는 바이러스 감염에 효과가 없으며, 부적절한 사용은 항생제 내성을 유발할 수 있습니다.\n",
        "\n",
        "위 내용을 한 문장으로 요약하세요:\"\"\"\n",
        "\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"요약된 내용:\", response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExGGGY10-QoR"
      },
      "source": [
        "Exercise: Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZbpuWcJ-QoR"
      },
      "source": [
        "### 1.2 Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLHWhjyH-QoR"
      },
      "outputs": [],
      "source": [
        "params = set_openai_params(temperature=0.7)\n",
        "prompt = \"\"\"다음 문맥에 기반하여 질문에 답하세요. 답변은 짧고 간결하게 작성하세요. 확신이 없으면 \"확실하지 않음\"이라고 답변하세요.\n",
        "\n",
        "문맥: 테플리주맙(Teplizumab)은 뉴저지의 제약회사인 오쏘 제약(Ortho Pharmaceutical)에서 개발되었습니다. 연구진은 항체의 초기 버전인 OKT3을 생성했습니다. 원래 쥐에서 얻어진 이 분자는 T 세포 표면에 결합하여 세포 살해 능력을 제한할 수 있었습니다. 1986년에 이 분자는 신장 이식 후 장기 거부 반응을 예방하기 위해 승인되었으며, 인간에게 사용이 허가된 최초의 치료용 항체로 기록되었습니다.\n",
        "\n",
        "질문: OKT3은 원래 무엇에서 얻어졌나요?\n",
        "\n",
        "답변:\"\"\"\n",
        "\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"요약된 내용:\", response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z24F-zd-QoR"
      },
      "source": [
        "Context obtained from here: https://www.nature.com/articles/d41586-023-00400-x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBRruLlq-QoR"
      },
      "source": [
        "Exercise: 프롬프트를 수정하여 모델이 답변에 대해 확신이 없다고 응답하도록 만드세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMnXzq8E-QoR"
      },
      "source": [
        "### 1.3 Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zk3jA4M-QoR"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"다음 텍스트를 중립적, 부정적, 긍정적으로 분류하세요.\n",
        "\n",
        "텍스트: 음식이 괜찮았던 것 같아요.\n",
        "\n",
        "감정:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"모델 응답:\", response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUmAt2ew-QoR"
      },
      "source": [
        "Exercise: 프롬프트를 수정하여 모델이 선택한 답변에 대한 설명을 제공하도록 설정해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"다음 텍스트를 중립적, 부정적, 긍정적으로 분류하고, 선택한 답변에 대한 이유를 설명하세요.\n",
        "\n",
        "텍스트: 음식이 괜찮았던 것 같아요.\n",
        "\n",
        "감정 및 설명:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"모델 응답:\", response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ],
      "metadata": {
        "id": "ygZ_N0Abl6eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCousjs-QoR"
      },
      "source": [
        "### 1.4 Role Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhclGIaN-QoS"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"다음은 AI 연구 보조원과의 대화입니다. 보조원의 말투는 기술적이고 과학적입니다.\n",
        "\n",
        "인간: 안녕하세요, 누구세요?\n",
        "AI: 안녕하세요! 저는 AI 연구 보조원입니다. 오늘 무엇을 도와드릴까요?\n",
        "인간: 블랙홀의 생성에 대해 알려줄 수 있나요?\n",
        "AI:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    display(Markdown(response))\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-2NlSfJ-QoS"
      },
      "source": [
        "Exercise: 프롬프트를 수정하여 모델이 간결하고 짧은 응답을 제공하도록 설정해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"다음은 AI 연구 보조원과의 대화입니다. 보조원의 말투는 기술적이고 과학적이며, 응답은 간결하고 짧게 유지됩니다.\n",
        "\n",
        "인간: 안녕하세요, 누구세요?\n",
        "AI: 안녕하세요! 저는 AI 연구 보조원입니다. 오늘 무엇을 도와드릴까요?\n",
        "인간: 블랙홀의 생성에 대해 알려줄 수 있나요?\n",
        "AI:\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    display(Markdown(response))\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ],
      "metadata": {
        "id": "iQK5902wmTs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpOhPA4E-QoS"
      },
      "source": [
        "### 1.5 Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_openai_params(temperature=0) # temperature를 0으로 낮춰 안정적인 응답생성을 유도합니다."
      ],
      "metadata": {
        "id": "h2-rtTiLRQ8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMQaAFC8-QoS"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"이 그룹의 홀수의 합이 짝수인지 확인하세요: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "문제를 단계별로 해결하세요. 먼저 홀수를 식별하고, 그들을 합산한 후 결과가 홀수인지 짝수인지 나타내세요.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    display(Markdown(response))\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVeiL61v-QoS"
      },
      "source": [
        "Exercise: 프롬프트 구조와 출력 형식을 개선하여 테스트해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"다음 숫자 그룹에서 홀수를 식별하고 합계를 계산한 후 결과가 홀수인지 짝수인지 확인하세요: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "출력은 다음 형식을 따르세요:\n",
        "1. 식별된 홀수:\n",
        "2. 홀수의 합:\n",
        "3. 결과(홀수 또는 짝수):\"\"\"\n"
      ],
      "metadata": {
        "id": "ugVIRzSgnDZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    display(Markdown(response))\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ],
      "metadata": {
        "id": "J4Se8XnGnMwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvm6awom-QoS"
      },
      "source": [
        "## 2. Advanced Prompting Techniques\n",
        "\n",
        "few-shot, chain of thoughts 등의 테크닉들을 다뤄봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fr4YY9q-QoS"
      },
      "source": [
        "### 2.2 Few-shot prompts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_openai_params(temperature=0)"
      ],
      "metadata": {
        "id": "bKSWIf1xTRKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW_3npQj-QoS"
      },
      "outputs": [],
      "source": [
        "# Zero-shot Prompting\n",
        "zero_shot_prompt = \"\"\"이 그룹의 홀수의 합이 짝수이면 True, 홀수이면 False로 응답하세요.:\n",
        "그룹: 15, 32, 5, 13, 82, 7, 1\n",
        "A:\"\"\"\n",
        "\n",
        "# Few-shot Prompting\n",
        "few_shot_prompt = \"\"\"이 그룹의 홀수의 합이 짝수이면 True, 홀수이면 False로 응답하세요. :\n",
        "\n",
        "그룹: 4, 8, 9, 15, 12, 2, 1\n",
        "A: 정답은 False입니다.\n",
        "\n",
        "그룹: 17, 10, 19, 4, 8, 12, 24\n",
        "A: 정답은 True입니다.\n",
        "\n",
        "그룹: 16, 11, 14, 4, 8, 13, 24\n",
        "A: 정답은 True입니다.\n",
        "\n",
        "그룹: 17, 9, 10, 12, 13, 4, 2\n",
        "A: 정답은 False입니다.\n",
        "\n",
        "그룹: 15, 32, 5, 13, 82, 7, 1\n",
        "A:\"\"\"\n",
        "\n",
        "# Zero-shot Prompting Test\n",
        "zero_shot_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": zero_shot_prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# Few-shot Prompting Test\n",
        "few_shot_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": few_shot_prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    # Zero-shot 응답\n",
        "    zero_shot_response = get_completion(params, zero_shot_prompt)\n",
        "    print(\"Zero-shot 응답:\")\n",
        "    print(zero_shot_response)\n",
        "\n",
        "    # Few-shot 응답\n",
        "    few_shot_response = get_completion(params, few_shot_prompt)\n",
        "    print(\"\\nFew-shot 응답:\")\n",
        "    print(few_shot_response)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few-shot prompting은 다양한 사례에서 유용하지만, 논리적 추론을 요구하는 경우에는 효과적이지 않을 수 있습니다."
      ],
      "metadata": {
        "id": "374jqOemS6uC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uYbdEnX-QoT"
      },
      "source": [
        "### 2.3 Chain-of-Thought (CoT) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_openai_params(temperature=0)"
      ],
      "metadata": {
        "id": "DoRHl_dFTkQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E63v_wpd-QoT"
      },
      "outputs": [],
      "source": [
        "# Chain-of-Thought (CoT) Prompting\n",
        "cot_prompt = \"\"\"이 그룹의 홀수의 합이 짝수이면 True, 홀수이면 False를 예측합니다.:\n",
        "그룹: 4, 8, 9, 15, 12, 2, 1\n",
        "A: 홀수 (9, 15, 1)를 모두 더하면 25입니다. 따라서 정답은 False입니다.\n",
        "\n",
        "그룹: 15, 32, 5, 13, 82, 7, 1\n",
        "A: \"\"\"\n",
        "\n",
        "cot_messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": cot_prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    # CoT Prompting 응답\n",
        "    cot_response = get_completion(params, cot_prompt)\n",
        "    print(\"Chain-of-Thought (CoT) 응답:\")\n",
        "    print(cot_response)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추론 능력은 LLM의 성능에 상당히 의존적입니다. 일반적으로 더 좋은 성능을 내는 gpt-4o로 모델을 변경합니다."
      ],
      "metadata": {
        "id": "9R_-KLdyTohF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # CoT Prompting 응답\n",
        "    cot_response = get_completion(set_openai_params(\n",
        "                                    model=\"gpt-4o\",\n",
        "                                    temperature=0), cot_prompt)\n",
        "    print(\"Chain-of-Thought (CoT) 응답:\")\n",
        "    print(cot_response)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")"
      ],
      "metadata": {
        "id": "osdvfbRTpPHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DBI5oZC-QoT"
      },
      "source": [
        "### 2.4 Zero-shot CoT\n",
        "\n",
        "**Zero-shot Chain-of-Thought (CoT)**란, 모델이 추가적인 예제 없이, 즉 사전 훈련된 정보만으로 단계적인 사고 과정을 통해 답을 도출하는 기법입니다. 이는 \"단계별로 생각해봅시다.\"(Let's think step by step)와 같은 특정한 프롬프트를 추가함으로써 모델이 문제를 논리적으로 해결하도록 유도하는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkUjPsAz-QoT"
      },
      "outputs": [],
      "source": [
        "# 문제 해결을 위한 프롬프트 (한국어)\n",
        "prompt = \"\"\"저는 시장에 가서 사과 10개를 샀습니다. 이웃에게 사과 2개를 주고 수리공에게도 2개를 주었습니다.\n",
        "이후 사과를 5개 더 사고 1개를 먹었습니다. 제가 가진 사과는 몇 개인가요?\n",
        "\n",
        "단계별로 생각해봅시다.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# OpenAI 응답 생성 함수 호출\n",
        "try:\n",
        "    response = get_completion(params, prompt)\n",
        "    print(\"AI 응답:\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcK2zjn9-QoW"
      },
      "source": [
        "### 2.5 Self-Consistency\n",
        "\n",
        "Self-Consistency는 Chain-of-Thought (CoT) 기반의 추론을 향상시키는 방법 중 하나로, 동일한 질문에 대해 여러 번 샘플링하여 가장 일관된 답을 선택하는 전략입니다.\n",
        "\n",
        "이 방법은 기본적인 CoT 방식보다 더 강력한 성능을 발휘할 수 있으며, 모델이 일관된 논리적 추론 경로를 찾도록 유도하는 역할을 합니다.\n",
        "\n",
        "문제: 한 사람의 하루 일정에서 시간이 겹치지 않도록 조정하고 최적의 스케줄을 계산하는 문제입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Self-Consistency 프롬프트\n",
        "prompt = \"\"\"아래의 스케줄을 확인하고 시간이 겹치지 않도록 최적의 스케줄을 작성하세요:\n",
        "\n",
        "1. 10:00 ~ 11:30 - 회의\n",
        "2. 11:00 ~ 12:00 - 점심 약속\n",
        "3. 13:00 ~ 14:30 - 팀 프로젝트\n",
        "4. 14:00 ~ 15:00 - 클라이언트 전화\n",
        "5. 16:00 ~ 17:00 - 개인 업무\n",
        "\n",
        "단계별로 생각하며, 가장 합리적인 스케줄을 작성하세요.\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }\n",
        "]\n",
        "\n",
        "# Self-Consistency를 위한 다중 생성\n",
        "try:\n",
        "    results = []\n",
        "    for _ in range(5):  # 5번 실행하여 다양한 스케줄 생성\n",
        "        response = get_completion(params, prompt)\n",
        "        results.append(response.strip())\n",
        "\n",
        "    # 모든 결과 출력\n",
        "    print(\"Self-Consistency 결과:\")\n",
        "    for idx, result in enumerate(results):\n",
        "        print(f\"{idx + 1}: {result}\")\n",
        "\n",
        "    # 가장 빈도 높은 스케줄 선택\n",
        "    final_schedule = max(set(results), key=results.count)\n",
        "    print(\"\\n최종 선택된 스케줄:\", final_schedule)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"오류 발생: {e}\")\n"
      ],
      "metadata": {
        "id": "6aefChI1sbbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Prompt Chaining\n",
        "\n",
        "Prompt Chaining은 하나의 프롬프트에서 얻은 결과를 다음 프롬프트의 입력으로 연결하여 더 정교한 응답을 생성하는 기법입니다.\n",
        "\n",
        "이를 통해 모델의 응답 품질을 향상시키고, 복잡한 문제를 단계별로 해결할 수 있습니다.\n",
        "\n",
        "예제로 어린이를 위한 재미있고 교육적인 이야기 생성해봅시다!"
      ],
      "metadata": {
        "id": "TjEO8euNsgUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_openai_params(\n",
        "        max_tokens=1024,\n",
        "        model='gpt-4o'\n",
        "    )"
      ],
      "metadata": {
        "id": "_c9gTeBgvWuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# 1. Persona 생성\n",
        "persona = \"\"\"당신은 어린이 잡지에 실릴 재미있고 교육적인 이야기를 쓰는 어린이 작가입니다.\n",
        "당신의 목표는 웃음을 주면서도 교훈을 전달하는 새로운 이야기를 작성하는 것입니다.\"\"\"\n",
        "\n",
        "# 2. Premise 생성\n",
        "premise_prompt = f\"\"\"{persona}\n",
        "\n",
        "토끼를 주인공으로 하는 어린이 이야기를 위한 한 문장의 전제를 작성하세요.\"\"\"\n",
        "\n",
        "# OpenAI 호출로 premise 생성\n",
        "premise_response = get_completion(params, premise_prompt)\n",
        "premise = premise_response.strip()\n",
        "\n",
        "display(Markdown(f\"### Premise:\\n{premise}\"))\n",
        "\n",
        "# 3. Outline 생성\n",
        "outline_prompt = f\"\"\"{persona}\n",
        "\n",
        "다음과 같은 전제를 가지고 있습니다:\n",
        "\n",
        "{premise}\n",
        "\n",
        "이야기의 플롯을 구성하는 5개의 핵심 포인트로 구성된 개요를 작성하세요.\"\"\"\n",
        "\n",
        "outline_response = get_completion(params, outline_prompt)\n",
        "outline = outline_response.strip()\n",
        "\n",
        "display(Markdown(f\"### Outline:\\n{outline}\"))\n",
        "\n",
        "# 4. 첫 번째 포인트에 해당하는 이야기만 생성해보기\n",
        "starting_prompt = f\"\"\"{persona}\n",
        "\n",
        "다음과 같은 전제를 가지고 있습니다:\n",
        "\n",
        "{premise}\n",
        "\n",
        "다음과 같은 이야기 개요를 가지고 있습니다:\n",
        "\n",
        "{outline}\n",
        "\n",
        "먼저 개요와 전제를 검토하세요. 개요의 첫 번째 포인트에 해당하는 이야기만 작성하세요.\n",
        "10문장을 작성하며, 다음 포인트는 다루지 마세요.\"\"\"\n",
        "\n",
        "starting_response = get_completion(params, starting_prompt)\n",
        "draft = starting_response.strip()\n",
        "\n",
        "display(Markdown(f\"### 이야기 초안 (첫 번째 포인트):\\n{draft}\"))\n",
        "\n",
        "# 5. 이야기 확장해 Draft를 늘려가보기 (Guidelines 추가)\n",
        "guidelines = \"\"\"작성 가이드라인:\n",
        "\n",
        "이야기를 가능한 한 풍부하게 작성하세요. 너무 빨리 스토리를 끝내면 안 됩니다.\n",
        "앞서 작성한 부분을 반복하지 말고, 개요의 다음 포인트로 확장하세요.\"\"\"\n",
        "\n",
        "continuation_prompt = f\"\"\"{persona}\n",
        "\n",
        "다음과 같은 전제를 가지고 있습니다:\n",
        "\n",
        "{premise}\n",
        "\n",
        "다음과 같은 이야기 개요를 가지고 있습니다:\n",
        "\n",
        "{outline}\n",
        "\n",
        "지금까지 작성된 이야기:\n",
        "\n",
        "{draft}\n",
        "\n",
        "=====\n",
        "먼저 전제와 개요, 그리고 지금까지 작성된 이야기를 검토하세요.\n",
        "\n",
        "다음 포인트에 해당하는 5문장을 작성하세요. 개요를 따라 진행하세요.\n",
        "\n",
        "하지만 이야기 작성을 끝났다면 마지막에 IAMDONE을 작성하세요.\n",
        "\n",
        "{guidelines}\"\"\"\n",
        "\n",
        "continuation_response = get_completion(params, continuation_prompt)\n",
        "continuation = continuation_response.strip()\n",
        "\n",
        "display(Markdown(f\"### 이야기 확장:\\n{continuation}\"))\n",
        "\n",
        "# 6. 반복적으로 이야기 확장하며 draft에 이야기를 추가하기\n",
        "draft += f\"\\n\\n{continuation}\"\n",
        "\n",
        "while \"IAMDONE\" not in continuation:\n",
        "    continuation_prompt = f\"\"\"{persona}\n",
        "\n",
        "    다음과 같은 전제를 가지고 있습니다:\n",
        "\n",
        "    {premise}\n",
        "\n",
        "    다음과 같은 이야기 개요를 가지고 있습니다:\n",
        "\n",
        "    {outline}\n",
        "\n",
        "    지금까지 작성된 이야기:\n",
        "\n",
        "    {draft}\n",
        "\n",
        "    =====\n",
        "    먼저 전제와 개요, 그리고 지금까지 작성된 이야기를 검토하세요.\n",
        "\n",
        "    개요의 다음 포인트에 해당하는 내용을 작성하세요.\n",
        "\n",
        "    모든 개요 포인트를 작성했다면, 마지막에 IAMDONE을 작성하세요.\n",
        "\n",
        "    {guidelines}\"\"\"\n",
        "\n",
        "    continuation_response = get_completion(params, continuation_prompt)\n",
        "    continuation = continuation_response.strip()\n",
        "    draft += f\"\\n\\n{continuation}\"\n",
        "\n",
        "# 7. 최종 이야기 정리\n",
        "final_story = draft.replace(\"IAMDONE\", \"\").strip()\n",
        "display(Markdown(f\"### 최종 이야기:\\n{final_story}\"))\n"
      ],
      "metadata": {
        "id": "jQ56wRQsv9G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "DAIR.AI | Elvis Saravia의 prompt engineering guide"
      ],
      "metadata": {
        "id": "OBXSrnZrawXr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKynItVl-QoW"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}